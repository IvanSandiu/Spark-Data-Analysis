{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN25C6ZyEeq+E9KCdv3i7uj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvanSandiu/Spark-Data-Analysis/blob/main/DataAnalysis_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis using Spark\n",
        "\n",
        "I started this project by loading employee data from a CSV file into a Spark DataFrame. From there, I explored the data step by step, applying different transformations and running SQL queries to answer questions like average salary, department distributions, and more.\n",
        "\n",
        "- Task 1: Generate DataFrame from CSV data.\n",
        "- Task 2: Define a schema for the data.\n",
        "- Task 3: Display schema of DataFrame.\n",
        "- Task 4: Create a temporary view.\n",
        "- Task 5: Execute an SQL query.\n",
        "- Task 6: Calculate Average Salary by Department.\n",
        "- Task 7: Filter and Display IT Department Employees.\n",
        "- Task 8: Add 10% Bonus to Salaries.\n",
        "- Task 9: Find Maximum Salary by Age.\n",
        "- Task 10: Self-Join on Employee Data.\n",
        "- Task 11: Calculate Average Employee Age.\n",
        "- Task 12: Calculate Total Salary by Department.\n",
        "- Task 13: Sort Data by Age and Salary.\n",
        "- Task 14: Count Employees in Each Department.\n",
        "- Task 15: Filter Employees with the letter o in the Name."
      ],
      "metadata": {
        "id": "rWeOzjjyD5NV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "\n",
        "Before starting, I made sure my environment had Python and Spark (PySpark) installed, and I also downloaded the CSV file with the employee data."
      ],
      "metadata": {
        "id": "nEtPoKehESca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKG3UkTODG20"
      },
      "outputs": [],
      "source": [
        "!pip install pysparkâ€¯ findspark wget\n",
        "\n",
        "import findspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "import wget\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "# Creating a SparkContext object\n",
        "sc = SparkContext.getOrCreate()\n",
        "# Creating a SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "wget.download(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/employees.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1: Generate a Spark DataFrame from the CSV data\n",
        "\n",
        "First, I read the employees.csv file into a Spark DataFrame called employees_df so I could start analyzing the data."
      ],
      "metadata": {
        "id": "GPqodxEpTNwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "NzrQFVrzTWvK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 2: Define a schema for the data\n",
        "\n",
        "Next, I defined a schema for the dataset to make sure Spark interpreted the data types correctly."
      ],
      "metadata": {
        "id": "F70S3vNLTYhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"date_column\", StringType(), True),\n",
        "    StructField(\"amount\", IntegerType(), True),\n",
        "    StructField(\"description\", StringType(), True),\n",
        "    StructField(\"location\", StringType(), True)\n",
        "])\n",
        "\n",
        "df1 = spark.read.csv(\"employees.csv\", header=True, schema=schema)"
      ],
      "metadata": {
        "id": "cQpIolL4TbRh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 3: Display schema of DataFrame\n",
        "\n",
        "I then displayed the schema of employees_df to confirm that all columns and data types looked correct."
      ],
      "metadata": {
        "id": "FHDS8LSXTnbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df.printSchema()"
      ],
      "metadata": {
        "id": "DwIqs16oTqlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 4: Create a temporary view\n",
        "\n",
        "Next, I created a temporary view called employees from the DataFrame so I could run SQL queries directly on the data.\n"
      ],
      "metadata": {
        "id": "FOCHEyEMTtax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df.createOrReplaceTempView(\"employees\")"
      ],
      "metadata": {
        "id": "-v2gvc0dTwIS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 5: Execute an SQL query\n",
        "\n",
        "I ran an SQL query on the employees view to retrieve only the employees older than 30. The query returned the filtered records as expected."
      ],
      "metadata": {
        "id": "vZ0LyMiNTxtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.sql(\"SELECT * FROM employees WHERE age > 30\")\n",
        "result.show()"
      ],
      "metadata": {
        "id": "CHb6trD_T0Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 6: Calculate Average Salary by Department\n",
        "\n",
        "Then, I wrote a query to calculate the average salary by department. This gave me a quick look at how salaries are distributed across different teams."
      ],
      "metadata": {
        "id": "gAZ1QK3vT4lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.sql(\"\"\"\n",
        "    SELECT department, AVG(salary) AS average_salary\n",
        "    FROM employees\n",
        "    GROUP BY department\n",
        "\"\"\")\n",
        "result.show()"
      ],
      "metadata": {
        "id": "fKk1AQ1JT_z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 7: Filter and Display IT Department Employees\n",
        "\n",
        "To focus specifically on the IT department, I filtered the DataFrame to show only those employees. This helped isolate their information from the rest."
      ],
      "metadata": {
        "id": "tw7wO9d2UBox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employeesIT = employees_df.filter(\"department = 'IT'\")\n",
        "employeesIT.show()"
      ],
      "metadata": {
        "id": "puR4CCzVUExI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 8: Add 10% Bonus to Salaries\n",
        "\n",
        "After that, I added a new column called SalaryAfterBonus. In this column, each salary includes a 10% bonus, letting me see the adjusted earnings for every employee."
      ],
      "metadata": {
        "id": "qE7_VVAiUGpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "employees_df_withBonus = employees_df.withColumn(\"SalaryAfterBonus\", col('salary')*1.1)\n",
        "employees_df_withBonus.show()"
      ],
      "metadata": {
        "id": "TjoriJtTUJMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 9: Find Maximum Salary by Age\n",
        "\n",
        "I grouped the data by employee age and calculated the maximum salary in each age group. This showed me the top earners for every age."
      ],
      "metadata": {
        "id": "T_wXdI7VUNmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "employees_df_maxSalary = employees_df.orderBy('age').groupBy('age').agg(max(\"salary\").alias(\"maximum_salary\"))\n",
        "employees_df_maxSalary.show()"
      ],
      "metadata": {
        "id": "zoVpQtJeUPnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 10: Self-Join on Employee Data\n",
        "\n",
        "I also performed a self-join on the DataFrame, matching the data with itself using the Emp_No column. This kind of operation can help when comparing employees or checking consistency."
      ],
      "metadata": {
        "id": "PZYCjhZbUS75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df_joined = employees_df.join(employees_df, 'Emp_No', 'inner')\n",
        "employees_df_joined.show()"
      ],
      "metadata": {
        "id": "5C14c_E2UVJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 11: Calculate Average Employee Age\n",
        "\n",
        "To understand the workforce better, I calculated the average age of all employees using an aggregation function."
      ],
      "metadata": {
        "id": "Fmr5fz0fUaOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "employees_df_averageAge = employees_df.agg(avg(\"age\").alias(\"Average_age\"))\n",
        "employees_df_averageAge.show()"
      ],
      "metadata": {
        "id": "019QttxsUceS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 12: Calculate Total Salary by Department\n",
        "\n",
        "I calculated the total salary for each department. This made it easy to see which departments had the largest combined payroll."
      ],
      "metadata": {
        "id": "1XeIodaNUeUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "employees_df_departmentSalary = employees_df.groupBy('department').agg(sum(\"salary\").alias(\"Total_salary\"))\n",
        "employees_df_departmentSalary.show()"
      ],
      "metadata": {
        "id": "dUso5a5ZUguC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 13: Sort Data by Age and Salary\n",
        "\n",
        "I sorted the DataFrame by age in ascending order, and within each age group, I sorted by salary in descending order. This way, younger employees appeared first, with the highest earners listed at the top of their age group."
      ],
      "metadata": {
        "id": "_kdI8NUNUjY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df_sorted = employees_df.orderBy(col(\"age\").asc(), col(\"salary\").desc())\n",
        "employees_df_sorted.show()"
      ],
      "metadata": {
        "id": "61s_blqQUlaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 14: Count Employees in Each Department\n",
        "\n",
        "I counted how many employees belong to each department, which gave me a clear picture of the department sizes."
      ],
      "metadata": {
        "id": "WF6Ld_wbUnYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "employees_df_departmentSalary = employees_df.groupBy('department').agg(count(\"emp_no\").alias(\"Total_employees\"))\n",
        "employees_df_departmentSalary.show()"
      ],
      "metadata": {
        "id": "lVTHt2ERUpLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 15: Filter Employees with the letter o in the Name\n",
        "\n",
        "Finally, I applied a filter to select employees whose names contain the letter \"o\". This was a simple string-based query to check for text conditions in the dataset."
      ],
      "metadata": {
        "id": "iWyoTrI0Utlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employeesIT = employees_df.filter(col(\"emp_name\").contains(\"o\"))\n",
        "employeesIT.show()"
      ],
      "metadata": {
        "id": "r8OY_acXUxmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}